{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from numpy import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_inputs(df, input_cols, seq_len=107): \n",
    "    \"\"\"One-hot encode inputs\"\"\"\n",
    "    results = np.empty(shape=(df.shape[0],seq_len,0))\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    for col in input_cols: \n",
    "        tokenizer.fit_on_texts(df['sequence'])\n",
    "        int_sequences = tokenizer.texts_to_sequences(df['sequence'])\n",
    "        results = np.append(results, to_categorical(int_sequences), axis=2)\n",
    "    trend_mat = np.tile(np.reshape(np.arange(seq_len)/100, (1,seq_len,1)),(df.shape[0],1,1))\n",
    "    results = np.append(results, trend_mat, axis=2)\n",
    "    return results\n",
    "\n",
    "#from Kaggle user\n",
    "def pandas_list_to_array(df):\n",
    "    \"\"\"\n",
    "    Input: dataframe of shape (x, y), containing list of length l\n",
    "    Return: np.array of shape (x, l, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.transpose(\n",
    "        np.array(df.values.tolist()),\n",
    "        (0, 2, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified code from d2l.ai to allow for different types of layer\n",
    "def get_params(num_hiddens, layer_type, num_inputs=None, num_outputs=None): \n",
    "\n",
    "    def normal(shape): \n",
    "        return torch.randn(size=shape) * 0.01\n",
    "\n",
    "    params = []\n",
    "    if layer_type == 'base':\n",
    "        params.append(normal((num_inputs, num_hiddens)))\n",
    "    \n",
    "    if layer_type != 'output':\n",
    "        params.append(normal((num_hiddens, num_hiddens)))\n",
    "        params.append(torch.zeros(num_hiddens))\n",
    "    \n",
    "    else:\n",
    "        params.append(normal((num_hiddens, num_outputs)))\n",
    "        params.append(torch.zeros(num_outputs))\n",
    "\n",
    "    # Attach gradients\n",
    "    for param in params: \n",
    "        param.requires_grad_(True)\n",
    "    \n",
    "    return params\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens): \n",
    "    return torch.zeros((batch_size, num_hiddens))\n",
    "\n",
    "def relu(x): \n",
    "    return x*(x>0)\n",
    "\n",
    "def identity(x): \n",
    "    return x\n",
    "\n",
    "#from Chapter 8 of d2l.ai\n",
    "def rnn(inputs, state, params, len_pred=68): \n",
    "    #X dim: (num_steps, batch_size, num_inputs)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    \n",
    "    for X in inputs: \n",
    "        H = torch.tanh(torch.mm(X, W_xh)+torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)\n",
    "\n",
    "#define layer class\n",
    "class layer: \n",
    "    def __init__(self, layer_type, num_inputs = None, num_hiddens = None, num_outputs = None,  activation = identity): \n",
    "        self.activation = activation\n",
    "        self.num_inputs, self.num_hiddens, self.num_outputs = num_inputs, num_hiddens, num_outputs\n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        if self.layer_type == 'base':\n",
    "            self.params = get_params(num_hiddens, num_inputs=num_inputs, layer_type='base')\n",
    "            self.W_xh, self.W_hh, self.b_h = self.params\n",
    "        elif self.layer_type == 'intermediate': \n",
    "            self.params = get_params(num_hiddens, layer_type='intermediate')\n",
    "            self.W_hh, self.b_h = self.params \n",
    "        elif self.layer_type == 'output': \n",
    "            self.params = get_params(num_hiddens, num_outputs=num_outputs, \n",
    "            layer_type='output')\n",
    "            self.W_hq, self.b_q = self.params\n",
    "            \n",
    "    def __call__(self, X =None, H=None): \n",
    "        return self.forward(X, H)\n",
    "        \n",
    "    def forward(self, X=None, H=None):\n",
    "        \n",
    "        if self.layer_type == 'base':\n",
    "            \n",
    "            return self.activation(torch.mm(X, self.W_xh)+torch.mm(H, self.W_hh) + self.b_h)\n",
    "        elif self.layer_type == 'intermediate':\n",
    "            return self.activation(torch.mm(H, self.W_hh)+self.b_h)\n",
    "        elif self.layer_type == 'output': \n",
    "            return self.activation(torch.mm(H, self.W_hq)+self.b_q)\n",
    "    \n",
    "    def begin_state(self, batch_size): \n",
    "        self.params = self.init_state(batch_size, self.num_hiddens)\n",
    "\n",
    "#define recurrent network (adapted from Chapter 8 of d2l.ai)\n",
    "class RNNModelScratch: \n",
    "    def __init__(self,get_params, init_state): \n",
    "        self.init_state = init_state\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "    \n",
    "    def __call__(self, X, state): \n",
    "        return self.forward(X, state)\n",
    "    \n",
    "    def add_layer(self, layer): \n",
    "        self.layers.append(layer)\n",
    "        self.params += layer.params\n",
    "    \n",
    "    def forward(self, XX, state): \n",
    "        #H = self.begin_state(batch_size)\n",
    "        outputs = []\n",
    "        H = state\n",
    "        for X in XX:\n",
    "            H = self.layers[0].forward(X, H)\n",
    "            for l in self.layers[1:-1]: \n",
    "                H = l(H=H)\n",
    "            outputs.append(self.layers[-1](H=H)) \n",
    "        return torch.cat(outputs, dim=0)\n",
    "        \n",
    "    def begin_state(self, batch_size): \n",
    "        return self.init_state(batch_size, self.layers[-2].num_hiddens)\n",
    "\n",
    "#from Chapter 8 of d2l.ai, clip gradient to avoid exploding gradient issues\n",
    "def grad_clipping(model, theta): \n",
    "    if isinstance(model, nn.Module): \n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "    else: \n",
    "        params = model.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta: \n",
    "        for param in params: \n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "#adapted from Chapter 8 of d2l.ai. One full epoch of training\n",
    "def train_epoch(model, train_iter, loss, updater, \n",
    "               use_random_iter, batch_size=50, len_pred=68): \n",
    "    metric = np.zeros(2)\n",
    "    state = model.begin_state(batch_size=batch_size)\n",
    "    for X, Y in train_iter(train_inputs[:,0:68,:], train_labels, batch_size): \n",
    "#         if state is None or use_random_iter: \n",
    "#             state = model.begin_state(batch_size=X.shape[1])\n",
    "#         else: \n",
    "#             if isinstance(model, nn.Module) and not isinstance(state, tuple): \n",
    "#                 state.detach_()\n",
    "#             else: \n",
    "#                 for s in state: \n",
    "#                     s.detach_()\n",
    "        y = Y.reshape((-1, Y.shape[2]))\n",
    "        y_hat = model(X, state)\n",
    "        l = loss(y_hat, y).mean()    #.long() converts to long data type\n",
    "        if isinstance(updater, torch.optim.Optimizer): \n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(model,1)\n",
    "            updater.step()\n",
    "        else: \n",
    "            l.backward()\n",
    "            grad_clipping(model,1)\n",
    "            updater(batch_size=1)\n",
    "        \n",
    "        metric += np.array([l.data, 1])\n",
    "    return metric[0] / metric[1], metric[1]\n",
    "\n",
    "# loss function for mRNA vaccine application\n",
    "def MCRMSE(y_hat, y): \n",
    "    mse_col = torch.mean(torch.square(y_hat - y),axis=0)\n",
    "    result = torch.mean(torch.sqrt(mse_col), axis=0)\n",
    "    return result\n",
    "\n",
    "#from Chapter 8 of d2l.ai. define updater for gradient descent, and apply train_epoch\n",
    "def train(model, train_iter, lr, num_epochs, use_random_iter=False): \n",
    "    loss = MCRMSE \n",
    "    updater = torch.optim.SGD(model.params, lr)\n",
    "    for epoch in range(num_epochs): \n",
    "        loss_, speed = train_epoch(model, train_iter, loss, updater, \n",
    "                                  use_random_iter)\n",
    "        print(loss_)\n",
    "\n",
    "# generator for reading data\n",
    "def data_iter(train_inputs, train_labels, batch_size): \n",
    "    n = train_inputs.shape[0]\n",
    "    d = train_inputs.shape[2]\n",
    "    q = train_labels.shape[2]\n",
    "    initial_index = list(range(train_inputs.shape[0]))\n",
    "    random.shuffle(initial_index)\n",
    "    \n",
    "    for i in range(n//batch_size):\n",
    "        idx = initial_index[(i*batch_size):((i+1)*batch_size)]\n",
    "        XX = train_inputs[idx]   \n",
    "        YY = train_labels[idx]\n",
    "        \n",
    "        X = XX.transpose((1,0,2))\n",
    "        Y = YY.transpose((1,0,2))\n",
    "        yield torch.Tensor(X), torch.Tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to vaccine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "data_dir = '/Users/carlgreen/Downloads/stanford-covid-vaccine/'\n",
    "train_df = pd.read_json(data_dir + 'train.json', lines=True)\n",
    "test_df = pd.read_json(data_dir + 'test.json', lines=True)\n",
    "train_df = train_df.query(\"signal_to_noise >= 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2097, 107, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = ['sequence', 'structure', 'predicted_loop_type']\n",
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n",
    "train_inputs = one_hot_encode_inputs(train_df, input_cols)\n",
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pandas_list_to_array(train_df[pred_cols])\n",
    "X_train = torch.Tensor(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "model = RNNModelScratch(get_params = get_params, \n",
    "                        init_state = init_rnn_state)\n",
    "model.add_layer(layer(layer_type='base', num_inputs=16, num_hiddens=128, activation=relu))\n",
    "model.add_layer(layer(layer_type='intermediate', num_hiddens=128, activation=relu))\n",
    "model.add_layer(layer(layer_type='intermediate', num_hiddens=128, activation=relu))\n",
    "model.add_layer(layer(layer_type='output', num_hiddens=128, num_outputs=5, activation=identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5636317918940288\n",
      "0.5078650989183565\n",
      "0.5052482318587419\n",
      "0.504944598529397\n",
      "0.5055699835463268\n",
      "0.5050962421952224\n",
      "0.5050553265141278\n",
      "0.5053050699757367\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "train(model, data_iter, lr=0.1, num_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not fitting well, not sure whether it's a problem with the implementation in this code, or whether a simple RNN like this simply cannot perform well. Should try to implement GRU or LSTM to see whether performance improves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
